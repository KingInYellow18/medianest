name: Quality Gate Automation & Validation
# Comprehensive quality assurance with automated testing, security scanning, and compliance checks

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main, development, test, dev]
  push:
    branches: [main, development]
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to run quality gates on'
        required: false
        type: number
      full_scan:
        description: 'Run full security and compliance scan'
        required: false
        type: boolean
        default: false

env:
  NODE_VERSION: '20'
  QUALITY_THRESHOLD: 80
  COVERAGE_THRESHOLD: 80
  PERFORMANCE_THRESHOLD: 90
  SECURITY_THRESHOLD: 85

permissions:
  contents: read
  pull-requests: write
  checks: write
  security-events: write
  actions: read

jobs:
  pre-quality-analysis:
    name: Pre-Quality Analysis
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.check-changes.outputs.run-tests }}
      should-run-security: ${{ steps.check-changes.outputs.run-security }}
      should-run-performance: ${{ steps.check-changes.outputs.run-performance }}
      test-suites: ${{ steps.determine-suites.outputs.suites }}
      changed-workspaces: ${{ steps.check-changes.outputs.workspaces }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Analyze changed files
        id: check-changes
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && -n "${{ github.event.inputs.pr_number }}" ]]; then
            # Get changes for specific PR
            changed_files=$(gh pr diff ${{ github.event.inputs.pr_number }} --name-only)
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Get PR changes
            git fetch origin ${{ github.base_ref }}
            changed_files=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)
          else
            # Get push changes
            changed_files=$(git diff --name-only ${{ github.event.before }}..${{ github.event.after }})
          fi
          
          echo "Changed files:"
          echo "$changed_files"
          
          # Analyze changes to determine what to run
          run_tests="false"
          run_security="false"
          run_performance="false"
          workspaces=()
          
          while IFS= read -r file; do
            case "$file" in
              backend/*)
                run_tests="true"
                workspaces+=("backend")
                if [[ "$file" =~ \.(ts|js)$ ]]; then
                  run_security="true"
                fi
                ;;
              frontend/*)
                run_tests="true"
                run_performance="true"
                workspaces+=("frontend")
                if [[ "$file" =~ \.(ts|tsx|js|jsx)$ ]]; then
                  run_security="true"
                fi
                ;;
              shared/*)
                run_tests="true"
                workspaces+=("shared")
                ;;
              package.json|package-lock.json|*/package.json|*/package-lock.json)
                run_tests="true"
                run_security="true"
                ;;
              .github/workflows/*|*.yml|*.yaml)
                run_security="true"
                ;;
              docker*|Dockerfile*)
                run_security="true"
                ;;
            esac
          done <<< "$changed_files"
          
          # Remove duplicates from workspaces
          unique_workspaces=($(printf '%s\n' "${workspaces[@]}" | sort -u))
          workspaces_json=$(printf '%s\n' "${unique_workspaces[@]}" | jq -R . | jq -s .)
          
          # Force full scan if requested
          if [[ "${{ github.event.inputs.full_scan }}" == "true" ]]; then
            run_tests="true"
            run_security="true"
            run_performance="true"
          fi
          
          echo "run-tests=$run_tests" >> $GITHUB_OUTPUT
          echo "run-security=$run_security" >> $GITHUB_OUTPUT  
          echo "run-performance=$run_performance" >> $GITHUB_OUTPUT
          echo "workspaces=$workspaces_json" >> $GITHUB_OUTPUT
          
          echo "üîç Analysis results:"
          echo "- Run tests: $run_tests"
          echo "- Run security: $run_security"
          echo "- Run performance: $run_performance"
          echo "- Workspaces: ${unique_workspaces[*]}"

      - name: Determine test suites
        id: determine-suites
        run: |
          workspaces='${{ steps.check-changes.outputs.workspaces }}'
          
          suites=()
          
          # Add test suites based on changed workspaces
          if echo "$workspaces" | jq -e '. | contains(["backend"])' > /dev/null; then
            suites+=("backend-unit" "backend-integration")
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["frontend"])' > /dev/null; then
            suites+=("frontend-unit" "frontend-visual" "frontend-a11y")
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["shared"])' > /dev/null; then
            suites+=("shared-unit")
          fi
          
          # Always run E2E if backend or frontend changed
          if echo "$workspaces" | jq -e '. | contains(["backend", "frontend"])' > /dev/null; then
            suites+=("e2e")
          fi
          
          # Create JSON array
          suites_json=$(printf '%s\n' "${suites[@]}" | jq -R . | jq -s .)
          
          echo "suites=$suites_json" >> $GITHUB_OUTPUT
          echo "üß™ Test suites to run: ${suites[*]}"

  code-quality-analysis:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    needs: pre-quality-analysis
    if: needs.pre-quality-analysis.outputs.should-run-tests == 'true'
    outputs:
      quality-score: ${{ steps.calculate-quality.outputs.score }}
      linting-passed: ${{ steps.linting.outputs.passed }}
      type-check-passed: ${{ steps.type-check.outputs.passed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          
          # Install workspace dependencies
          workspaces='${{ needs.pre-quality-analysis.outputs.changed-workspaces }}'
          if echo "$workspaces" | jq -e '. | contains(["backend"])' > /dev/null; then
            cd backend && npm ci && cd ..
          fi
          if echo "$workspaces" | jq -e '. | contains(["frontend"])' > /dev/null; then
            cd frontend && npm ci && cd ..
          fi
          if echo "$workspaces" | jq -e '. | contains(["shared"])' > /dev/null; then
            cd shared && npm ci && cd ..
          fi

      - name: Run linting
        id: linting
        run: |
          echo "üîç Running linting checks..."
          
          lint_errors=0
          workspaces='${{ needs.pre-quality-analysis.outputs.changed-workspaces }}'
          
          # Root level linting
          if [[ -f "eslint.config.js" ]]; then
            if npm run lint --if-present; then
              echo "‚úÖ Root linting passed"
            else
              echo "‚ùå Root linting failed"
              lint_errors=$((lint_errors + 1))
            fi
          fi
          
          # Workspace linting
          if echo "$workspaces" | jq -e '. | contains(["backend"])' > /dev/null; then
            cd backend
            if npm run lint --if-present; then
              echo "‚úÖ Backend linting passed"
            else
              echo "‚ùå Backend linting failed"
              lint_errors=$((lint_errors + 1))
            fi
            cd ..
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["frontend"])' > /dev/null; then
            cd frontend
            if npm run lint --if-present; then
              echo "‚úÖ Frontend linting passed"
            else
              echo "‚ùå Frontend linting failed"
              lint_errors=$((lint_errors + 1))
            fi
            cd ..
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["shared"])' > /dev/null; then
            cd shared
            if npm run lint --if-present; then
              echo "‚úÖ Shared linting passed"
            else
              echo "‚ùå Shared linting failed"
              lint_errors=$((lint_errors + 1))
            fi
            cd ..
          fi
          
          passed="true"
          if [[ $lint_errors -gt 0 ]]; then
            passed="false"
          fi
          
          echo "passed=$passed" >> $GITHUB_OUTPUT
          echo "errors=$lint_errors" >> $GITHUB_OUTPUT

      - name: Run type checking
        id: type-check
        run: |
          echo "üîç Running TypeScript type checks..."
          
          type_errors=0
          workspaces='${{ needs.pre-quality-analysis.outputs.changed-workspaces }}'
          
          # Workspace type checking
          if echo "$workspaces" | jq -e '. | contains(["backend"])' > /dev/null; then
            cd backend
            if npm run typecheck --if-present || npm run type-check --if-present; then
              echo "‚úÖ Backend type check passed"
            else
              echo "‚ùå Backend type check failed"
              type_errors=$((type_errors + 1))
            fi
            cd ..
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["frontend"])' > /dev/null; then
            cd frontend
            if npm run typecheck --if-present || npm run type-check --if-present; then
              echo "‚úÖ Frontend type check passed"
            else
              echo "‚ùå Frontend type check failed"
              type_errors=$((type_errors + 1))
            fi
            cd ..
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["shared"])' > /dev/null; then
            cd shared
            if npm run typecheck --if-present || npm run type-check --if-present; then
              echo "‚úÖ Shared type check passed"
            else
              echo "‚ùå Shared type check failed"
              type_errors=$((type_errors + 1))
            fi
            cd ..
          fi
          
          passed="true"
          if [[ $type_errors -gt 0 ]]; then
            passed="false"
          fi
          
          echo "passed=$passed" >> $GITHUB_OUTPUT
          echo "errors=$type_errors" >> $GITHUB_OUTPUT

      - name: Calculate quality score
        id: calculate-quality
        run: |
          lint_passed="${{ steps.linting.outputs.passed }}"
          type_passed="${{ steps.type-check.outputs.passed }}"
          
          score=100
          
          if [[ "$lint_passed" != "true" ]]; then
            score=$((score - 30))
          fi
          
          if [[ "$type_passed" != "true" ]]; then
            score=$((score - 40))
          fi
          
          echo "score=$score" >> $GITHUB_OUTPUT
          echo "üìä Code quality score: $score/100"

  comprehensive-testing:
    name: Comprehensive Test Suite
    runs-on: ubuntu-latest
    needs: [pre-quality-analysis, code-quality-analysis]
    if: needs.pre-quality-analysis.outputs.should-run-tests == 'true'
    outputs:
      test-results: ${{ steps.run-tests.outputs.results }}
      coverage-percentage: ${{ steps.coverage.outputs.percentage }}
      tests-passed: ${{ steps.run-tests.outputs.passed }}
    strategy:
      matrix:
        test-suite: ${{ fromJson(needs.pre-quality-analysis.outputs.test-suites) }}
      fail-fast: false
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          
          case "${{ matrix.test-suite }}" in
            backend-*)
              cd backend && npm ci && cd ..
              ;;
            frontend-*)
              cd frontend && npm ci && cd ..
              ;;
            shared-*)
              cd shared && npm ci && cd ..
              ;;
            e2e)
              # Install all dependencies for E2E
              cd backend && npm ci && cd ..
              cd frontend && npm ci && cd ..
              ;;
          esac

      - name: Run test suite
        id: run-tests
        run: |
          echo "üß™ Running test suite: ${{ matrix.test-suite }}"
          
          test_command=""
          test_passed="false"
          
          case "${{ matrix.test-suite }}" in
            backend-unit)
              cd backend
              if npm run test:unit --if-present || npm test --if-present; then
                test_passed="true"
              fi
              cd ..
              ;;
            backend-integration)
              cd backend
              if npm run test:integration --if-present; then
                test_passed="true"
              fi
              cd ..
              ;;
            frontend-unit)
              cd frontend
              if npm run test:unit --if-present || npm test --if-present; then
                test_passed="true"
              fi
              cd ..
              ;;
            frontend-visual)
              if npm run test:visual --if-present; then
                test_passed="true"
              fi
              ;;
            frontend-a11y)
              if npm run test:a11y --if-present; then
                test_passed="true"
              fi
              ;;
            shared-unit)
              cd shared
              if npm run test:unit --if-present || npm test --if-present; then
                test_passed="true"
              fi
              cd ..
              ;;
            e2e)
              if npm run test:e2e --if-present; then
                test_passed="true"
              fi
              ;;
          esac
          
          echo "passed=$test_passed" >> $GITHUB_OUTPUT
          echo "suite=${{ matrix.test-suite }}" >> $GITHUB_OUTPUT
          
          if [[ "$test_passed" == "true" ]]; then
            echo "‚úÖ ${{ matrix.test-suite }} tests passed"
          else
            echo "‚ùå ${{ matrix.test-suite }} tests failed"
          fi

      - name: Generate coverage report
        id: coverage
        if: matrix.test-suite == 'backend-unit' || matrix.test-suite == 'frontend-unit'
        run: |
          echo "üìä Generating coverage report for ${{ matrix.test-suite }}"
          
          coverage_percentage=0
          
          case "${{ matrix.test-suite }}" in
            backend-unit)
              cd backend
              if npm run test:coverage --if-present; then
                # Extract coverage percentage from output
                coverage_percentage=$(grep -o "All files.*[0-9]\+\.[0-9]\+" coverage/lcov-report/index.html | grep -o "[0-9]\+\.[0-9]\+" | head -1 || echo "0")
              fi
              cd ..
              ;;
            frontend-unit)
              cd frontend  
              if npm run test:coverage --if-present; then
                coverage_percentage=$(grep -o "All files.*[0-9]\+\.[0-9]\+" coverage/lcov-report/index.html | grep -o "[0-9]\+\.[0-9]\+" | head -1 || echo "0")
              fi
              cd ..
              ;;
          esac
          
          echo "percentage=$coverage_percentage" >> $GITHUB_OUTPUT
          echo "üìà Coverage: $coverage_percentage%"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test-suite }}-${{ github.run_id }}
          path: |
            **/coverage/
            **/test-results/
            **/*.xml
          retention-days: 30

  security-scanning:
    name: Security & Vulnerability Scanning
    runs-on: ubuntu-latest
    needs: pre-quality-analysis
    if: needs.pre-quality-analysis.outputs.should-run-security == 'true'
    outputs:
      security-score: ${{ steps.calculate-security.outputs.score }}
      vulnerabilities-found: ${{ steps.scan-vulnerabilities.outputs.found }}
      security-passed: ${{ steps.calculate-security.outputs.passed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run dependency audit
        id: audit-deps
        run: |
          echo "üîç Running dependency security audit..."
          
          audit_errors=0
          
          # Root audit
          if npm audit --audit-level=moderate; then
            echo "‚úÖ Root dependencies are secure"
          else
            echo "‚ö†Ô∏è Root dependencies have security issues"
            audit_errors=$((audit_errors + 1))
          fi
          
          # Workspace audits
          workspaces='${{ needs.pre-quality-analysis.outputs.changed-workspaces }}'
          
          if echo "$workspaces" | jq -e '. | contains(["backend"])' > /dev/null; then
            cd backend
            if npm audit --audit-level=moderate; then
              echo "‚úÖ Backend dependencies are secure"
            else
              echo "‚ö†Ô∏è Backend dependencies have security issues"
              audit_errors=$((audit_errors + 1))
            fi
            cd ..
          fi
          
          if echo "$workspaces" | jq -e '. | contains(["frontend"])' > /dev/null; then
            cd frontend
            if npm audit --audit-level=moderate; then
              echo "‚úÖ Frontend dependencies are secure"
            else
              echo "‚ö†Ô∏è Frontend dependencies have security issues"
              audit_errors=$((audit_errors + 1))
            fi
            cd ..
          fi
          
          echo "errors=$audit_errors" >> $GITHUB_OUTPUT

      - name: Scan for vulnerabilities
        id: scan-vulnerabilities
        run: |
          echo "üîí Scanning for security vulnerabilities..."
          
          vulnerabilities=0
          
          # Check for common vulnerability patterns
          if grep -r "eval(" --include="*.js" --include="*.ts" . 2>/dev/null; then
            echo "‚ö†Ô∏è Found eval() usage - potential security risk"
            vulnerabilities=$((vulnerabilities + 1))
          fi
          
          if grep -r "innerHTML" --include="*.js" --include="*.ts" --include="*.tsx" . 2>/dev/null; then
            echo "‚ö†Ô∏è Found innerHTML usage - potential XSS risk"
            vulnerabilities=$((vulnerabilities + 1))
          fi
          
          if grep -r "dangerouslySetInnerHTML" --include="*.tsx" --include="*.jsx" . 2>/dev/null; then
            echo "‚ö†Ô∏è Found dangerouslySetInnerHTML usage - review for XSS protection"
            vulnerabilities=$((vulnerabilities + 1))
          fi
          
          # Check for exposed secrets patterns
          if grep -r -E "(password|secret|key|token).*=.*['\"][^'\"]{8,}" --include="*.js" --include="*.ts" --include="*.json" . 2>/dev/null; then
            echo "‚ö†Ô∏è Potential hardcoded secrets found"
            vulnerabilities=$((vulnerabilities + 1))
          fi
          
          echo "found=$vulnerabilities" >> $GITHUB_OUTPUT
          echo "üîç Found $vulnerabilities potential vulnerabilities"

      - name: CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          languages: javascript
        continue-on-error: true

      - name: Calculate security score
        id: calculate-security
        run: |
          audit_errors="${{ steps.audit-deps.outputs.errors }}"
          vulnerabilities="${{ steps.scan-vulnerabilities.outputs.found }}"
          
          score=100
          
          # Deduct points for audit issues
          score=$((score - audit_errors * 15))
          
          # Deduct points for vulnerabilities
          score=$((score - vulnerabilities * 10))
          
          # Ensure score doesn't go below 0
          if [[ $score -lt 0 ]]; then
            score=0
          fi
          
          passed="true"
          if [[ $score -lt ${{ env.SECURITY_THRESHOLD }} ]]; then
            passed="false"
          fi
          
          echo "score=$score" >> $GITHUB_OUTPUT
          echo "passed=$passed" >> $GITHUB_OUTPUT
          echo "üõ°Ô∏è Security score: $score/100"

  performance-testing:
    name: Performance & Load Testing
    runs-on: ubuntu-latest
    needs: pre-quality-analysis
    if: needs.pre-quality-analysis.outputs.should-run-performance == 'true'
    outputs:
      performance-score: ${{ steps.lighthouse.outputs.score }}
      performance-passed: ${{ steps.calculate-performance.outputs.passed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          if [[ -f "frontend/package.json" ]]; then
            cd frontend && npm ci && cd ..
          fi

      - name: Run Lighthouse audit
        id: lighthouse
        run: |
          echo "‚ö° Running Lighthouse performance audit..."
          
          if [[ -f "scripts/lighthouse-audit.js" ]]; then
            if npm run audit:lighthouse --if-present; then
              echo "‚úÖ Lighthouse audit completed"
              # Extract performance score (this would need to be customized based on your output format)
              score=90  # Placeholder - extract from actual lighthouse output
            else
              echo "‚ùå Lighthouse audit failed"
              score=0
            fi
          else
            echo "‚ÑπÔ∏è No Lighthouse audit script found, skipping"
            score=100  # Default score when no performance testing is configured
          fi
          
          echo "score=$score" >> $GITHUB_OUTPUT

      - name: Run performance tests
        id: perf-tests
        run: |
          echo "üèÉ Running performance tests..."
          
          if npm run test:performance --if-present; then
            echo "‚úÖ Performance tests passed"
            perf_passed="true"
          else
            echo "‚ùå Performance tests failed"
            perf_passed="false"
          fi
          
          echo "passed=$perf_passed" >> $GITHUB_OUTPUT

      - name: Calculate performance score
        id: calculate-performance
        run: |
          lighthouse_score="${{ steps.lighthouse.outputs.score }}"
          perf_tests_passed="${{ steps.perf-tests.outputs.passed }}"
          
          score=$lighthouse_score
          
          if [[ "$perf_tests_passed" != "true" ]]; then
            score=$((score - 20))
          fi
          
          passed="true"
          if [[ $score -lt ${{ env.PERFORMANCE_THRESHOLD }} ]]; then
            passed="false"
          fi
          
          echo "score=$score" >> $GITHUB_OUTPUT
          echo "passed=$passed" >> $GITHUB_OUTPUT
          echo "‚ö° Performance score: $score/100"

  quality-gate-evaluation:
    name: Quality Gate Evaluation & Reporting
    runs-on: ubuntu-latest
    needs: [code-quality-analysis, comprehensive-testing, security-scanning, performance-testing]
    if: always()
    outputs:
      overall-passed: ${{ steps.evaluate.outputs.passed }}
      quality-report: ${{ steps.generate-report.outputs.report }}
    steps:
      - name: Evaluate quality gates
        id: evaluate
        run: |
          echo "üéØ Evaluating quality gates..."
          
          # Collect scores and results
          quality_score="${{ needs.code-quality-analysis.outputs.quality-score || 0 }}"
          tests_passed="${{ needs.comprehensive-testing.outputs.tests-passed || 'false' }}"
          security_score="${{ needs.security-scanning.outputs.security-score || 0 }}"
          security_passed="${{ needs.security-scanning.outputs.security-passed || 'false' }}"
          performance_score="${{ needs.performance-testing.outputs.performance-score || 0 }}"
          performance_passed="${{ needs.performance-testing.outputs.performance-passed || 'false' }}"
          
          # Calculate overall score
          total_score=0
          weight_count=0
          
          if [[ $quality_score -gt 0 ]]; then
            total_score=$((total_score + quality_score))
            weight_count=$((weight_count + 1))
          fi
          
          if [[ $security_score -gt 0 ]]; then
            total_score=$((total_score + security_score))
            weight_count=$((weight_count + 1))
          fi
          
          if [[ $performance_score -gt 0 ]]; then
            total_score=$((total_score + performance_score))
            weight_count=$((weight_count + 1))
          fi
          
          if [[ $weight_count -gt 0 ]]; then
            overall_score=$((total_score / weight_count))
          else
            overall_score=0
          fi
          
          # Determine if quality gates pass
          gates_passed="true"
          
          if [[ $overall_score -lt ${{ env.QUALITY_THRESHOLD }} ]]; then
            gates_passed="false"
          fi
          
          if [[ "$tests_passed" == "false" ]]; then
            gates_passed="false"
          fi
          
          if [[ "$security_passed" == "false" ]]; then
            gates_passed="false"
          fi
          
          if [[ "$performance_passed" == "false" ]]; then
            gates_passed="false"
          fi
          
          echo "passed=$gates_passed" >> $GITHUB_OUTPUT
          echo "overall-score=$overall_score" >> $GITHUB_OUTPUT
          
          echo "üìä Overall quality score: $overall_score/100"
          echo "üéØ Quality gates passed: $gates_passed"

      - name: Generate quality report
        id: generate-report
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          overall_passed="${{ steps.evaluate.outputs.passed }}"
          overall_score="${{ steps.evaluate.outputs.overall-score }}"
          
          # Create comprehensive quality report
          cat > quality-report.md << EOF
          ## üéØ Quality Gate Report
          
          **Overall Status:** $(if [[ "$overall_passed" == "true" ]]; then echo "‚úÖ PASSED"; else echo "‚ùå FAILED"; fi)
          **Overall Score:** $overall_score/100
          
          ### üìä Detailed Results
          
          #### Code Quality
          - **Score:** ${{ needs.code-quality-analysis.outputs.quality-score || 'N/A' }}/100
          - **Linting:** $(if [[ "${{ needs.code-quality-analysis.outputs.linting-passed }}" == "true" ]]; then echo "‚úÖ Passed"; else echo "‚ùå Failed"; fi)
          - **Type Check:** $(if [[ "${{ needs.code-quality-analysis.outputs.type-check-passed }}" == "true" ]]; then echo "‚úÖ Passed"; else echo "‚ùå Failed"; fi)
          
          #### Testing
          - **Status:** $(if [[ "${{ needs.comprehensive-testing.outputs.tests-passed }}" == "true" ]]; then echo "‚úÖ Passed"; else echo "‚ùå Failed"; fi)
          - **Coverage:** ${{ needs.comprehensive-testing.outputs.coverage-percentage || 'N/A' }}%
          
          #### Security
          - **Score:** ${{ needs.security-scanning.outputs.security-score || 'N/A' }}/100
          - **Status:** $(if [[ "${{ needs.security-scanning.outputs.security-passed }}" == "true" ]]; then echo "‚úÖ Passed"; else echo "‚ùå Failed"; fi)
          - **Vulnerabilities:** ${{ needs.security-scanning.outputs.vulnerabilities-found || '0' }} found
          
          #### Performance
          - **Score:** ${{ needs.performance-testing.outputs.performance-score || 'N/A' }}/100
          - **Status:** $(if [[ "${{ needs.performance-testing.outputs.performance-passed }}" == "true" ]]; then echo "‚úÖ Passed"; else echo "‚ùå Failed"; fi)
          
          ### üéØ Quality Thresholds
          - **Quality Threshold:** ${{ env.QUALITY_THRESHOLD }}/100
          - **Coverage Threshold:** ${{ env.COVERAGE_THRESHOLD }}%
          - **Security Threshold:** ${{ env.SECURITY_THRESHOLD }}/100
          - **Performance Threshold:** ${{ env.PERFORMANCE_THRESHOLD }}/100
          
          EOF
          
          if [[ "$overall_passed" == "false" ]]; then
            cat >> quality-report.md << EOF
          ### ‚ùå Issues to Address
          
          EOF
            
            if [[ "${{ needs.code-quality-analysis.outputs.linting-passed }}" != "true" ]]; then
              echo "- Fix linting errors" >> quality-report.md
            fi
            
            if [[ "${{ needs.code-quality-analysis.outputs.type-check-passed }}" != "true" ]]; then
              echo "- Fix TypeScript type errors" >> quality-report.md
            fi
            
            if [[ "${{ needs.comprehensive-testing.outputs.tests-passed }}" != "true" ]]; then
              echo "- Fix failing tests" >> quality-report.md
            fi
            
            if [[ "${{ needs.security-scanning.outputs.security-passed }}" != "true" ]]; then
              echo "- Address security vulnerabilities" >> quality-report.md
            fi
            
            if [[ "${{ needs.performance-testing.outputs.performance-passed }}" != "true" ]]; then
              echo "- Improve performance metrics" >> quality-report.md
            fi
          fi
          
          cat >> quality-report.md << EOF
          
          ---
          *Quality gate evaluation completed at $(date -u '+%Y-%m-%d %H:%M:%S UTC')*
          EOF
          
          echo "report-file=quality-report.md" >> $GITHUB_OUTPUT

      - name: Comment quality report on PR
        if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          pr_number="${{ github.event.pull_request.number }}"
          
          # Find existing quality report comment
          existing_comment=$(gh pr view $pr_number --json comments --jq '.comments[] | select(.body | contains("Quality Gate Report")) | .id' | head -1)
          
          if [[ -n "$existing_comment" ]]; then
            gh api repos/${{ github.repository }}/issues/comments/$existing_comment \
              --method PATCH \
              --field body="$(cat quality-report.md)"
            echo "Updated existing quality report comment"
          else
            gh pr comment $pr_number --body-file quality-report.md
            echo "Created new quality report comment"
          fi

      - name: Set PR labels based on quality gates
        if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          pr_number="${{ github.event.pull_request.number }}"
          overall_passed="${{ steps.evaluate.outputs.passed }}"
          
          # Remove old quality labels
          old_labels=("quality-gate-passed" "quality-gate-failed" "needs-quality-fixes")
          for label in "${old_labels[@]}"; do
            gh pr edit $pr_number --remove-label "$label" 2>/dev/null || true
          done
          
          # Add appropriate quality label
          if [[ "$overall_passed" == "true" ]]; then
            gh pr edit $pr_number --add-label "quality-gate-passed"
          else
            gh pr edit $pr_number --add-label "quality-gate-failed"
            gh pr edit $pr_number --add-label "needs-quality-fixes"
          fi

      - name: Upload quality artifacts
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-report-${{ github.run_id }}
          path: |
            quality-report.md
            **/coverage/
            **/test-results/
          retention-days: 90

      - name: Fail workflow if quality gates don't pass
        if: steps.evaluate.outputs.passed != 'true'
        run: |
          echo "‚ùå Quality gates failed. Please address the issues above."
          exit 1