name: Documentation QA & Testing

on:
  pull_request:
    paths:
      - 'docs/**'
      - 'mkdocs.yml'
      - '*.md'
  push:
    branches: [main, develop]
    paths:
      - 'docs/**'
      - 'mkdocs.yml'
      - '*.md'
  workflow_dispatch:
    inputs:
      qa_level:
        description: 'QA Testing Level'
        required: true
        default: 'standard'
        type: choice
        options:
        - quick
        - standard
        - comprehensive
      check_external_links:
        description: 'Check external links (slow)'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '20'

jobs:
  # Content validation and linting
  content-validation:
    name: Content Validation
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install validation tools
      run: |
        pip install \
          mkdocs-material \
          markdownlint-cli2 \
          yamllint \
          proselint \
          textstat \
          requests \
          beautifulsoup4

    - name: Validate Markdown files
      run: |
        echo "🔍 Validating Markdown content..."
        
        # Find all markdown files
        find docs/ -name "*.md" > markdown-files.list
        
        if [[ ! -s markdown-files.list ]]; then
          echo "No Markdown files found in docs/"
          exit 1
        fi
        
        echo "Found $(wc -l < markdown-files.list) Markdown files"
        
        # Check for basic markdown structure
        echo "Checking basic Markdown structure..."
        error_count=0
        
        while IFS= read -r file; do
          echo "Checking: $file"
          
          # Check for required elements
          if ! grep -q "^# " "$file"; then
            echo "⚠️  Missing main heading (H1) in $file"
            ((error_count++))
          fi
          
          # Check for empty files
          if [[ ! -s "$file" ]]; then
            echo "❌ Empty file: $file"
            ((error_count++))
          fi
          
          # Check for TODO markers
          if grep -q "TODO\|FIXME\|XXX" "$file"; then
            echo "⚠️  Contains TODO markers: $file"
            grep -n "TODO\|FIXME\|XXX" "$file" | head -3
          fi
          
          # Check for broken internal links
          if grep -q "](.*\.md)" "$file"; then
            echo "Checking internal links in $file..."
            grep -o "](.*\.md)" "$file" | sed 's/](\(.*\))/\1/' | while read -r link; do
              # Convert relative paths to absolute
              if [[ "$link" =~ ^\.\.?/ ]]; then
                # Handle relative paths
                link_path="$(dirname "$file")/$link"
                link_path="$(realpath "$link_path" 2>/dev/null || echo "$link_path")"
              elif [[ "$link" =~ ^/ ]]; then
                # Absolute path from docs root
                link_path="docs$link"
              else
                # Same directory
                link_path="$(dirname "$file")/$link"
              fi
              
              if [[ ! -f "$link_path" ]]; then
                echo "❌ Broken internal link in $file: $link -> $link_path"
                ((error_count++))
              fi
            done
          fi
          
        done < markdown-files.list
        
        echo "Content validation completed with $error_count errors"
        if [[ $error_count -gt 10 ]]; then
          echo "❌ Too many content errors ($error_count). Please fix critical issues."
          exit 1
        fi

    - name: YAML configuration validation
      run: |
        echo "🔍 Validating YAML configurations..."
        
        # Validate mkdocs.yml
        echo "Checking mkdocs.yml..."
        yamllint mkdocs.yml || echo "⚠️  YAML linting issues in mkdocs.yml"
        
        # Validate MkDocs config
        mkdocs config-validation
        
        echo "✅ YAML validation completed"

    - name: Style and readability check
      run: |
        echo "📖 Checking documentation style and readability..."
        
        # Create a simple readability report
        cat > readability-check.py << 'EOF'
        import os
        import re
        import textstat
        from pathlib import Path
        
        def analyze_file(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Remove markdown formatting for analysis
            text = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
            text = re.sub(r'`[^`]+`', '', text)
            text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
            text = re.sub(r'[#*_]', '', text)
            
            if len(text.strip()) < 100:
                return None
            
            return {
                'file': filepath,
                'words': len(text.split()),
                'sentences': textstat.sentence_count(text),
                'reading_time': round(len(text.split()) / 200, 1),  # 200 WPM average
                'flesch_score': textstat.flesch_reading_ease(text),
                'grade_level': textstat.flesch_kincaid_grade(text)
            }
        
        docs_dir = Path('docs')
        reports = []
        
        for md_file in docs_dir.rglob('*.md'):
            result = analyze_file(md_file)
            if result:
                reports.append(result)
        
        # Generate report
        print("# 📖 Documentation Readability Report")
        print()
        print("| File | Words | Reading Time | Flesch Score | Grade Level |")
        print("|------|-------|--------------|--------------|-------------|")
        
        total_words = 0
        difficult_files = []
        
        for report in sorted(reports, key=lambda x: x['words'], reverse=True):
            total_words += report['words']
            grade_level = report['grade_level']
            flesch_score = report['flesch_score']
            
            # Flag difficult content
            if grade_level > 12 or flesch_score < 30:
                difficult_files.append(report['file'])
            
            print(f"| {report['file']} | {report['words']} | {report['reading_time']}min | {flesch_score:.1f} | {grade_level:.1f} |")
        
        print()
        print(f"**Total Documentation:** {total_words:,} words")
        print(f"**Estimated Reading Time:** {round(total_words / 200, 1)} minutes")
        
        if difficult_files:
            print()
            print("⚠️  **Files that may be difficult to read:**")
            for file in difficult_files:
                print(f"- {file}")
        
        EOF
        
        python readability-check.py > readability-report.md || echo "Readability check failed"

    - name: Accessibility check
      run: |
        echo "♿ Checking documentation accessibility..."
        
        # Check for accessibility issues in Markdown
        cat > accessibility-check.py << 'EOF'
        import os
        import re
        from pathlib import Path
        
        def check_accessibility(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.readlines()
            
            issues = []
            
            for i, line in enumerate(content, 1):
                # Check for images without alt text
                img_matches = re.findall(r'!\[([^\]]*)\]\([^)]+\)', line)
                for alt_text in img_matches:
                    if not alt_text.strip():
                        issues.append(f"Line {i}: Image missing alt text")
                
                # Check for links without descriptive text
                link_matches = re.findall(r'\[([^\]]+)\]\([^)]+\)', line)
                for link_text in link_matches:
                    if link_text.lower() in ['here', 'click here', 'read more', 'link']:
                        issues.append(f"Line {i}: Non-descriptive link text: '{link_text}'")
                
                # Check heading structure (basic)
                if line.startswith('###') and not any(content[j].startswith('##') for j in range(max(0, i-10), i)):
                    issues.append(f"Line {i}: H3 without parent H2")
            
            return issues
        
        docs_dir = Path('docs')
        all_issues = []
        
        for md_file in docs_dir.rglob('*.md'):
            issues = check_accessibility(md_file)
            if issues:
                all_issues.extend([f"{md_file}: {issue}" for issue in issues])
        
        print("# ♿ Accessibility Check Report")
        print()
        
        if all_issues:
            print("⚠️  **Accessibility Issues Found:**")
            print()
            for issue in all_issues[:20]:  # Limit output
                print(f"- {issue}")
            if len(all_issues) > 20:
                print(f"- ... and {len(all_issues) - 20} more issues")
        else:
            print("✅ No accessibility issues detected")
        
        print(f"\n**Total Issues:** {len(all_issues)}")
        
        EOF
        
        python accessibility-check.py > accessibility-report.md

    - name: Generate content analysis report
      run: |
        echo "📊 Generating comprehensive content analysis..."
        
        # Count files and content
        total_files=$(find docs/ -name "*.md" | wc -l)
        total_words=$(find docs/ -name "*.md" -exec cat {} \; | wc -w)
        total_images=$(find docs/ -name "*.md" -exec grep -h "!\[" {} \; | wc -l)
        total_links=$(find docs/ -name "*.md" -exec grep -h "\](" {} \; | wc -l)
        
        cat > content-analysis.md << EOF
        # 📊 Content Analysis Report
        
        **Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        **Commit:** ${{ github.sha }}
        
        ## Overview
        - **Total Markdown Files:** $total_files
        - **Total Words:** $total_words
        - **Total Images:** $total_images
        - **Total Links:** $total_links
        - **Estimated Reading Time:** $(( total_words / 200 )) minutes
        
        ## File Structure
        \`\`\`
        $(find docs/ -type f -name "*.md" | sort)
        \`\`\`
        
        ## Largest Files (by word count)
        \`\`\`
        $(find docs/ -name "*.md" -exec wc -w {} + | sort -nr | head -10)
        \`\`\`
        
        EOF

    - name: Upload analysis reports
      uses: actions/upload-artifact@v4
      with:
        name: content-analysis-reports
        path: |
          readability-report.md
          accessibility-report.md
          content-analysis.md
          markdown-files.list
        retention-days: 30

  # Build testing with multiple configurations
  build-testing:
    name: Build Testing
    runs-on: ubuntu-latest
    strategy:
      matrix:
        config: [strict, standard, minimal]
        include:
          - config: strict
            mkdocs_opts: "--strict --verbose"
            description: "Strict mode with all warnings as errors"
          - config: standard
            mkdocs_opts: "--verbose"
            description: "Standard build configuration"
          - config: minimal
            mkdocs_opts: "--clean"
            description: "Minimal build (clean only)"
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install MkDocs and dependencies
      run: |
        pip install \
          mkdocs-material \
          mkdocs-git-revision-date-localized-plugin \
          mkdocs-minify-plugin \
          mkdocs-redirects \
          mkdocs-tags \
          mkdocs-social \
          pygments \
          pymdown-extensions

    - name: Test build - ${{ matrix.config }}
      run: |
        echo "🔨 Testing ${{ matrix.config }} build configuration..."
        echo "Description: ${{ matrix.description }}"
        
        # Clean any previous builds
        rm -rf site/
        
        # Run build with specific configuration
        echo "Running: mkdocs build ${{ matrix.mkdocs_opts }}"
        mkdocs build ${{ matrix.mkdocs_opts }}
        
        # Validate build output
        if [[ ! -d "site" ]]; then
          echo "❌ Build directory 'site' not created"
          exit 1
        fi
        
        if [[ ! -f "site/index.html" ]]; then
          echo "❌ Main index.html not found"
          exit 1
        fi
        
        # Count generated files
        html_files=$(find site/ -name "*.html" | wc -l)
        css_files=$(find site/ -name "*.css" | wc -l)
        js_files=$(find site/ -name "*.js" | wc -l)
        
        echo "✅ Build successful (${{ matrix.config }})"
        echo "   HTML files: $html_files"
        echo "   CSS files: $css_files"
        echo "   JS files: $js_files"
        echo "   Total size: $(du -sh site/ | cut -f1)"

    - name: Upload build artifacts - ${{ matrix.config }}
      uses: actions/upload-artifact@v4
      with:
        name: build-test-${{ matrix.config }}
        path: site/
        retention-days: 7

  # Performance and optimization testing
  performance-testing:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: [build-testing]
    steps:
    - name: Download build artifact
      uses: actions/download-artifact@v4
      with:
        name: build-test-standard
        path: site/

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}

    - name: Install performance testing tools
      run: |
        npm install -g lighthouse
        pip install requests beautifulsoup4

    - name: Start local server
      run: |
        cd site/
        python -m http.server 8080 &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        sleep 5

    - name: Run Lighthouse audit
      run: |
        echo "🔍 Running Lighthouse performance audit..."
        
        # Run Lighthouse on key pages
        pages=("" "api/" "user/" "development/")
        
        for page in "${pages[@]}"; do
          echo "Auditing: http://localhost:8080/$page"
          
          lighthouse "http://localhost:8080/$page" \
            --output json \
            --output-path "lighthouse-$page.json" \
            --chrome-flags="--headless --no-sandbox" \
            --only-categories=performance,accessibility,best-practices,seo \
            --preset=desktop || echo "Lighthouse audit failed for $page"
        done

    - name: Analyze performance results
      run: |
        echo "📊 Analyzing performance results..."
        
        cat > analyze-performance.py << 'EOF'
        import json
        import os
        from pathlib import Path
        
        def analyze_lighthouse_report(filepath):
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                
                categories = data.get('categories', {})
                audits = data.get('audits', {})
                
                # Extract key metrics
                metrics = {
                    'performance': categories.get('performance', {}).get('score', 0),
                    'accessibility': categories.get('accessibility', {}).get('score', 0),
                    'best_practices': categories.get('best-practices', {}).get('score', 0),
                    'seo': categories.get('seo', {}).get('score', 0),
                    'first_contentful_paint': audits.get('first-contentful-paint', {}).get('displayValue', 'N/A'),
                    'largest_contentful_paint': audits.get('largest-contentful-paint', {}).get('displayValue', 'N/A'),
                    'cumulative_layout_shift': audits.get('cumulative-layout-shift', {}).get('displayValue', 'N/A'),
                    'total_blocking_time': audits.get('total-blocking-time', {}).get('displayValue', 'N/A')
                }
                
                return metrics
            except Exception as e:
                print(f"Error analyzing {filepath}: {e}")
                return None
        
        # Find all lighthouse reports
        reports = list(Path('.').glob('lighthouse-*.json'))
        
        print("# 🚀 Performance Analysis Report")
        print()
        print("| Page | Performance | Accessibility | Best Practices | SEO | FCP | LCP |")
        print("|------|-------------|---------------|----------------|-----|-----|-----|")
        
        total_performance = 0
        total_accessibility = 0
        page_count = 0
        
        for report_file in reports:
            page_name = str(report_file).replace('lighthouse-', '').replace('.json', '').replace('.', 'Home')
            if not page_name:
                page_name = 'Home'
            
            metrics = analyze_lighthouse_report(report_file)
            if metrics:
                page_count += 1
                total_performance += metrics['performance'] or 0
                total_accessibility += metrics['accessibility'] or 0
                
                print(f"| {page_name} | {metrics['performance']:.2f} | {metrics['accessibility']:.2f} | {metrics['best_practices']:.2f} | {metrics['seo']:.2f} | {metrics['first_contentful_paint']} | {metrics['largest_contentful_paint']} |")
        
        if page_count > 0:
            print()
            print(f"**Average Performance Score:** {total_performance/page_count:.2f}")
            print(f"**Average Accessibility Score:** {total_accessibility/page_count:.2f}")
            
            # Performance assessment
            avg_perf = total_performance / page_count
            if avg_perf >= 0.9:
                print("🟢 **Performance Status: EXCELLENT**")
            elif avg_perf >= 0.7:
                print("🟡 **Performance Status: GOOD**")
            else:
                print("🔴 **Performance Status: NEEDS IMPROVEMENT**")
        
        EOF
        
        python analyze-performance.py > performance-report.md

    - name: Check bundle sizes
      run: |
        echo "📦 Analyzing bundle sizes..."
        
        # Analyze CSS and JS bundle sizes
        find site/ -name "*.css" -exec ls -lh {} \; | sort -k5 -hr > css-sizes.txt
        find site/ -name "*.js" -exec ls -lh {} \; | sort -k5 -hr > js-sizes.txt
        
        echo "## 📦 Bundle Size Analysis" >> performance-report.md
        echo "" >> performance-report.md
        echo "### CSS Files" >> performance-report.md
        echo '```' >> performance-report.md
        head -10 css-sizes.txt >> performance-report.md || echo "No CSS files found" >> performance-report.md
        echo '```' >> performance-report.md
        echo "" >> performance-report.md
        echo "### JavaScript Files" >> performance-report.md
        echo '```' >> performance-report.md
        head -10 js-sizes.txt >> performance-report.md || echo "No JS files found" >> performance-report.md
        echo '```' >> performance-report.md

    - name: Cleanup server
      if: always()
      run: |
        if [[ -n "$SERVER_PID" ]]; then
          kill $SERVER_PID || true
        fi

    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      with:
        name: performance-reports
        path: |
          performance-report.md
          lighthouse-*.json
          css-sizes.txt
          js-sizes.txt
        retention-days: 30

  # Cross-platform testing
  cross-platform-testing:
    name: Cross-Platform Build Test
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.10', '3.11', '3.12']
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        pip install mkdocs-material mkdocs-git-revision-date-localized-plugin \
                    mkdocs-minify-plugin mkdocs-redirects \
                    mkdocs-tags mkdocs-social pygments \
                    pymdown-extensions

    - name: Test build on ${{ matrix.os }}
      run: |
        echo "Testing MkDocs build on ${{ matrix.os }} with Python ${{ matrix.python-version }}"
        mkdocs build --clean --verbose
        
        # Verify build
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          if [ ! -d "site" ]; then echo "Build failed"; exit 1; fi
        else
          if [[ ! -d "site" ]]; then echo "Build failed"; exit 1; fi
        fi
        
        echo "✅ Build successful on ${{ matrix.os }}"

  # Security and vulnerability testing
  security-testing:
    name: Security Testing
    runs-on: ubuntu-latest
    needs: [build-testing]
    steps:
    - name: Download build artifact
      uses: actions/download-artifact@v4
      with:
        name: build-test-standard
        path: site/

    - name: Security scan of built documentation
      run: |
        echo "🔒 Running security scan on documentation..."
        
        # Check for potential XSS vectors in HTML
        find site/ -name "*.html" -exec grep -l "javascript:" {} \; > potential-xss.txt || true
        
        # Check for sensitive information
        find site/ -name "*.html" -exec grep -i "password\|secret\|token\|api.key" {} \; > sensitive-info.txt || true
        
        # Check for insecure links
        find site/ -name "*.html" -exec grep -o "http://[^\"']*" {} \; > insecure-links.txt || true
        
        # Generate security report
        cat > security-report.md << EOF
        # 🔒 Security Scan Report
        
        **Scan Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        
        ## Findings
        
        ### Potential XSS Vectors
        $(if [[ -s potential-xss.txt ]]; then echo "⚠️  Found files with javascript: URLs:"; cat potential-xss.txt; else echo "✅ No javascript: URLs found"; fi)
        
        ### Sensitive Information
        $(if [[ -s sensitive-info.txt ]]; then echo "⚠️  Found potential sensitive information:"; head -5 sensitive-info.txt; else echo "✅ No obvious sensitive information found"; fi)
        
        ### Insecure Links
        $(if [[ -s insecure-links.txt ]]; then echo "⚠️  Found insecure HTTP links:"; head -10 insecure-links.txt | sort -u; else echo "✅ No insecure HTTP links found"; fi)
        
        EOF

    - name: Upload security report
      uses: actions/upload-artifact@v4
      with:
        name: security-scan-report
        path: |
          security-report.md
          potential-xss.txt
          sensitive-info.txt
          insecure-links.txt
        retention-days: 30

  # Comprehensive QA summary
  qa-summary:
    name: QA Summary
    runs-on: ubuntu-latest
    needs: [content-validation, build-testing, performance-testing, cross-platform-testing, security-testing]
    if: always()
    steps:
    - name: Generate QA summary
      run: |
        echo "# 🔍 Documentation QA Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**QA Run:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status | Description |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|-------------|" >> $GITHUB_STEP_SUMMARY
        echo "| Content Validation | ${{ needs.content-validation.result }} | Markdown linting and validation |" >> $GITHUB_STEP_SUMMARY
        echo "| Build Testing | ${{ needs.build-testing.result }} | Multi-configuration build tests |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Testing | ${{ needs.performance-testing.result }} | Lighthouse audits and optimization |" >> $GITHUB_STEP_SUMMARY
        echo "| Cross-Platform Testing | ${{ needs.cross-platform-testing.result }} | Multi-OS and Python version tests |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Testing | ${{ needs.security-testing.result }} | Security vulnerability scans |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Overall status
        if [[ "${{ needs.content-validation.result }}" == "success" && \
              "${{ needs.build-testing.result }}" == "success" && \
              "${{ needs.performance-testing.result }}" == "success" && \
              "${{ needs.cross-platform-testing.result }}" == "success" && \
              "${{ needs.security-testing.result }}" == "success" ]]; then
          echo "🟢 **Overall QA Status: PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "🔴 **Overall QA Status: ISSUES DETECTED**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Artifacts Generated" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- 📊 Content Analysis Reports" >> $GITHUB_STEP_SUMMARY
        echo "- 🚀 Performance Analysis Reports" >> $GITHUB_STEP_SUMMARY
        echo "- 🔒 Security Scan Results" >> $GITHUB_STEP_SUMMARY
        echo "- 🔨 Build Artifacts (Multiple Configurations)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "[📥 Download all artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY

    - name: QA completion notification
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        echo "🎉 Documentation QA completed for main branch"
        echo "All quality assurance checks have been completed"
        echo "Check the summary above for detailed results"